import asyncio
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib 
import os 
import time

from services.kis.data import kis_data
from ai.models import StockLSTM
from ai.utils import add_indicators

# --- [ê¸´ê¸‰ ìˆ˜ì •: ì†ë„ ë° í•™ìŠµ íš¨ìœ¨ ìµœì í™”] ---
SEQ_LENGTH = 20       # [ì¶•ì†Œ] 60ì¼ -> 20ì¼ (ê³¼ê±° 1ë‹¬ì¹˜ë§Œ ë´„, ì†ë„ í–¥ìƒ)
PREDICT_DAY = 1      
TARGET_PCT = 0.01    
EPOCHS = 100         
LR = 0.001           
BATCH_SIZE = 1024     # [ì¦ê°€] 64 -> 1024 (CPU ì—°ì‚° íš¨ìœ¨ ê·¹ëŒ€í™”)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

def create_dataset_multiclass(data_x, data_y, seq_length):
    xs, ys = [], []
    for i in range(len(data_x) - seq_length):
        x = data_x[i:(i + seq_length)]
        y = data_y[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

async def run_training(market_name, stock_list, model_file, scaler_file):
    print(f"\nğŸš€ [{market_name}] ìƒìœ„ {len(stock_list)}ê°œ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ (Fast Mode)...")
    print(f"ğŸ’» í•™ìŠµ ì¥ì¹˜: {DEVICE} | ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {BATCH_SIZE}")
    
    all_x = []
    all_y = []
    
    # 1. ë°ì´í„° ìˆ˜ì§‘
    for idx, stock in enumerate(stock_list):
        # ì§„í–‰ìƒí™©ì„ 10ê°œ ë‹¨ìœ„ë¡œë§Œ ì¶œë ¥ (ë¡œê·¸ ì¤„ì„)
        if idx % 10 == 0:
            print(f"[{idx+1}/{len(stock_list)}] {stock['name']} ìˆ˜ì§‘ ì¤‘...")
            
        await asyncio.sleep(0.01) # ë”œë ˆì´ ìµœì†Œí™”
        
        chart_data = await kis_data.get_stock_chart(stock['market'], stock['code'], "D")
        if not chart_data or len(chart_data) < 250: continue
            
        df = pd.DataFrame(chart_data)
        try:
            df = add_indicators(df)
            
            df['Return'] = df['close'].shift(-PREDICT_DAY) / df['close'] - 1.0
            df.dropna(inplace=True)
            
            conditions = [
                (df['Return'] <= -TARGET_PCT),
                (df['Return'] > -TARGET_PCT) & (df['Return'] < TARGET_PCT),
                (df['Return'] >= TARGET_PCT)
            ]
            choices = [0, 1, 2]
            df['Target'] = np.select(conditions, choices, default=1)
            
            features = ['Change', 'RSI', 'Disparity_5', 'Disparity_20', 'Vol_Ratio', 'PPO', 'BB_Width']
            
            data_x = df[features].values
            data_y = df['Target'].values
            
            x_seq, y_seq = create_dataset_multiclass(data_x, data_y, SEQ_LENGTH)
            all_x.append(x_seq)
            all_y.append(y_seq)
        except: continue

    if not all_x:
        print(f"âŒ [{market_name}] ë°ì´í„° ì—†ìŒ")
        return

    X = np.concatenate(all_x, axis=0)
    Y = np.concatenate(all_y, axis=0)
    
    count_0 = np.sum(Y == 0)
    count_1 = np.sum(Y == 1)
    count_2 = np.sum(Y == 2)
    total_samples = len(Y)
    
    print(f"ğŸ“Š ë°ì´í„°: ì´ {total_samples}ê°œ (í•˜ë½ {count_0} | íš¡ë³´ {count_1} | ìƒìŠ¹ {count_2})")
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ë„ˆë¬´ ê·¹ë‹¨ì ì´ì§€ ì•Šê²Œ ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš© ê³ ë ¤ ê°€ëŠ¥í•˜ë‚˜ ì¼ë‹¨ ìœ ì§€)
    w0 = total_samples / (3 * count_0) if count_0 > 0 else 1.0
    w1 = total_samples / (3 * count_1) if count_1 > 0 else 1.0
    w2 = total_samples / (3 * count_2) if count_2 > 0 else 1.0
    class_weights = torch.FloatTensor([w0, w1, w2]).to(DEVICE)
    
    # ìŠ¤ì¼€ì¼ë§
    num_samples, seq_len, num_features = X.shape
    X_reshaped = X.reshape(-1, num_features)
    scaler = MinMaxScaler(feature_range=(-1, 1))
    X_scaled = scaler.fit_transform(X_reshaped)
    X_final = X_scaled.reshape(num_samples, seq_len, num_features)
    joblib.dump(scaler, os.path.join(BASE_DIR, scaler_file))

    # DataLoader
    x_tensor = torch.tensor(X_final, dtype=torch.float32).to(DEVICE)
    y_tensor = torch.tensor(Y, dtype=torch.long).to(DEVICE)
    dataset = TensorDataset(x_tensor, y_tensor)
    
    train_size = int(len(dataset) * 0.8)
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # [ìˆ˜ì •] ëª¨ë¸ ê²½ëŸ‰í™”: Hidden 64, Layers 2
    model = StockLSTM(input_size=num_features, hidden_size=64, num_layers=2, output_size=3, dropout=0.2).to(DEVICE)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘...")
    
    start_time = time.time()
    for epoch in range(EPOCHS):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        avg_train_loss = total_loss / len(train_loader)
        
        # ê²€ì¦ (ë§¤ ì—í¬í¬)
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for val_x, val_y in val_loader:
                val_out = model(val_x)
                loss = criterion(val_out, val_y)
                val_loss += loss.item()
                
                _, predicted = torch.max(val_out.data, 1)
                total += val_y.size(0)
                correct += (predicted == val_y).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100 * correct / total
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        
        elapsed = time.time() - epoch_start
        lr_msg = f" | ğŸ“‰ LR: {new_lr:.5f}" if new_lr != old_lr else ""
        
        # [ì¤‘ìš”] Lossê°€ 1.09 ë°‘ìœ¼ë¡œ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸
        print(f"Ep {epoch+1:3d}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | Acc: {val_acc:.2f}% ({elapsed:.1f}s){lr_msg}")

    print(f"âœ… í•™ìŠµ ì™„ë£Œ ({(time.time()-start_time)/60:.1f}ë¶„ ì†Œìš”)")
    torch.save(model.state_dict(), os.path.join(BASE_DIR, model_file))

async def main():
    # êµ­ë‚´
    kr_list = []
    try:
        ranks = await kis_data.get_ranking_data("cap")
        limit = 200 if len(ranks) > 200 else len(ranks)
        for item in ranks[:limit]:
            kr_list.append({"market": "KR", "code": item['code'], "name": item['name']})
    except: pass
    
    if kr_list: await run_training("êµ­ë‚´(KR)", kr_list, "stock_model_kr.pth", "scaler_kr.pkl")

    # ë‚˜ìŠ¤ë‹¥
    nas_list = []
    try:
        ranks = await kis_data.get_overseas_ranking_data("market_cap", "NAS")
        limit = 200 if len(ranks) > 200 else len(ranks)
        for item in ranks[:limit]:
            nas_list.append({"market": "NAS", "code": item['code'], "name": item.get('name', item['code'])})
    except: pass

    if nas_list: await run_training("ë‚˜ìŠ¤ë‹¥(NAS)", nas_list, "stock_model_nas.pth", "scaler_nas.pkl")

if __name__ == "__main__":
    asyncio.run(main())